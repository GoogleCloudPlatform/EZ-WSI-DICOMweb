{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9swiCsR_grx"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.sandbox.google.com/github/Google-Health/imaging-research/blob/master/path-foundation/linear-classifier-demo.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/Google-Health/imaging-research/tree/master/path-foundation\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCy7vXMN73tI"
      },
      "source": [
        "~~~\n",
        "Copyright 2024 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "~~~\n",
        "# Train a Digital Pathology Linear Classifier From Images Stored on DICOM\n",
        "This notebook is a demonstration of generating and using embeddings from the Path Foundation API to train a linear classifier. This API enables users to compute embeddings for histopathology images. The contents include how to build an API request to generate embeddings from stored patches and train a linear model using the embeddings. Note: This notebook is for API demonstration purposes only. As with all machine-learning use-cases it is critical to consider training and evaluation datasets that reflect the expected distribution of the intended use case.\n",
        "\n",
        "**Additional details**: For this demo, whole slide images (WSIs) available from the dataset below were split into train and evaluation sets. A subset of patches were sampled randomly from across all available slides and embeddings were generated via the Path Foundation model.\n",
        "\n",
        "**Dataset**: This notebook uses the [CAMELYON16](https://camelyon16.grand-challenge.org/) dataset, which contains WSIs from lymph node specimens with and without metastatic breast cancer. Any work that uses this dataset should consider additional details along with usage and citation requirements listed on [their website](https://camelyon17.grand-challenge.org/Data/).\n",
        "\n",
        "**Dataset citation**: Babak Ehteshami Bejnordi; Mitko Veta; Paul Johannes van Diest; Bram van Ginneken; Nico Karssemeijer; Geert Litjens; Jeroen A. W. M. van der Laak; and the CAMELYON16 Consortium. Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer. JAMA. 2017;318(22):2199â€“2210. DOI: 10.1001/jama.2017.14585\n",
        "# Prerequisites\n",
        "You must have access to the Pathology Foundation Tool. See the project's [README](https://github.com/Google-Health/imaging-research/blob/master/path-foundation/README.md) for details.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv80cFw676RO"
      },
      "source": [
        "\n",
        "## Imports and constants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCCYlzPChsFO"
      },
      "outputs": [],
      "source": [
        "# @title Pip install EZ-WSI DICOMweb\n",
        "%%capture\n",
        "!pip install --upgrade ez_wsi_dicomweb>=6.0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK3W9QHunOkS"
      },
      "outputs": [],
      "source": [
        "# @title Authenticate Colab User.\n",
        "from google.colab import auth\n",
        "# There will be a popup asking you to sign in with your user account and approve\n",
        "# access.\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_wez6RRhvSK"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import concurrent.futures\n",
        "from dataclasses import dataclass\n",
        "import functools\n",
        "import json\n",
        "import random\n",
        "from typing import Iterator, List, Mapping, Sequence, Tuple\n",
        "import warnings\n",
        "from ez_wsi_dicomweb import credential_factory\n",
        "from ez_wsi_dicomweb import dicom_slide\n",
        "from ez_wsi_dicomweb import dicom_web_interface\n",
        "from ez_wsi_dicomweb import patch_embedding\n",
        "from ez_wsi_dicomweb import patch_embedding_endpoints\n",
        "from ez_wsi_dicomweb import patch_embedding_types\n",
        "from ez_wsi_dicomweb import pixel_spacing\n",
        "from ez_wsi_dicomweb.ml_toolkit import dicom_path\n",
        "import google.cloud.storage\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.linear_model\n",
        "import sklearn.metrics\n",
        "import sklearn.model_selection\n",
        "import sklearn.pipeline\n",
        "import sklearn.preprocessing\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XPfdfgS8T3Q"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "PROJECT_ID = 'hai-cd3-foundations'\n",
        "BUCKET_NAME = 'hai-cd3-foundations-pathology-vault-entry'\n",
        "DATASET_PROJECT_ID = 'hai-cd3-foundations'\n",
        "DATASET_LOCATION = 'us-west1'\n",
        "DATASET_ID = 'pathology'\n",
        "STORE_ID = 'camelyon'\n",
        "PATCHES_DIR_NAME = 'patches/'\n",
        "EMBEDDINGs_DIR_NAME = 'embeddings/'\n",
        "CANCER_FILE = 'all_cancer_patches.json'\n",
        "BENIGN_FILE = 'all_non_cancer_patches.json'\n",
        "TRAINING_CANCER_PATCH_COUNT = 250  # @param {type: 'integer'}\n",
        "TRAINING_BENIGN_PATCH_COUNT = 250  # @param {type: 'integer'}\n",
        "EVAL_CANCER_PATCH_COUNT = 50  # @param {type: 'integer'}\n",
        "EVAL_BENIGN_PATCH_COUNT = 50  # @param {type: 'integer'}\n",
        "PATCH_SIZE = 224\n",
        "TARGET_PIXEL_SPACING = pixel_spacing.PixelSpacing.FromMagnificationString('20X')\n",
        "EVAL_RESERVED_SLIDES = (\n",
        "    EVAL_CANCER_PATCH_COUNT + 15\n",
        ")  # slides reserved for the eval set. Add some buffer in case patch count is much higher than the reserved slide count."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gewPN660pKht"
      },
      "source": [
        " ## Additional setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luYgDjz-9ZHy"
      },
      "outputs": [],
      "source": [
        "# Helper function to render patches, this function is used to display example patches\n",
        "\n",
        "\n",
        "# Use patch location and DICOM information from a returned embedding to retrieve and display the correct patch\n",
        "def render_patch_from_embedding(\n",
        "    patch: dicom_slide.DicomPatch, plot_name: str = ''\n",
        ") -> None:\n",
        "  patch_bytes = patch.image_bytes()\n",
        "  plt.figure(figsize=(2, 2))\n",
        "  plt.imshow(patch_bytes)\n",
        "  plt.title(plot_name)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPPs6S69tEwc"
      },
      "source": [
        "## Download & Organize Patches Into Train and Eval Lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHJ5DvKj8ZiT"
      },
      "outputs": [],
      "source": [
        "# @title Downloads pre-defined patch coordinates to sample\n",
        "client = google.cloud.storage.Client()\n",
        "bucket = google.cloud.storage.Bucket(client, name=BUCKET_NAME)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Patch:\n",
        "  \"\"\"Patch metadata stored on GCS.\"\"\"\n",
        "  slide_id: str\n",
        "  study_instance_uid: str\n",
        "  series_instance_uid: str\n",
        "  x_origin: int\n",
        "  y_origin: int\n",
        "\n",
        "\n",
        "def download_and_convert_patches(blob_path: str) -> List[Patch]:\n",
        "  \"\"\"Downloads a blob and converts JSON to dataclass\"\"\"\n",
        "  json_str = client.bucket(BUCKET_NAME).get_blob(blob_path).download_as_string()\n",
        "  return [Patch(**pd) for pd in json.loads(json_str)['patches']]\n",
        "\n",
        "\n",
        "# Downloads patch coordiantes\n",
        "cancer_patch_coordiantes = download_and_convert_patches(\n",
        "    PATCHES_DIR_NAME + CANCER_FILE\n",
        ")\n",
        "benign_patch_coordiantes = download_and_convert_patches(\n",
        "    PATCHES_DIR_NAME + BENIGN_FILE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6GzRCJiBGVL"
      },
      "outputs": [],
      "source": [
        "# @title Split into Training and Eval lists\n",
        "# Split by slide for eval and separate patches into training and eval lists\n",
        "# according to patch labels.\n",
        "\n",
        "\n",
        "# Bucket patches by slide_id\n",
        "def build_patches_by_slide_id(\n",
        "    patch_collection: Sequence[Patch],\n",
        ") -> Mapping[str, List[Patch]]:\n",
        "  patches_by_slide = defaultdict(list)  # Create a defaultdict of lists\n",
        "  for patch in patch_collection:\n",
        "    patches_by_slide[patch.slide_id].append(patch)  # Directly append\n",
        "  return patches_by_slide\n",
        "\n",
        "\n",
        "def select_random_slide_ids(\n",
        "    patches_by_slide: Mapping[str, Sequence[Patch]], num_slides: int\n",
        ") -> List[str]:\n",
        "  slide_ids = list(patches_by_slide)  # Get all slide IDs\n",
        "  random.shuffle(slide_ids)  # Shuffle for randomness\n",
        "  return slide_ids[:num_slides]  # Select the first num_slides elements\n",
        "\n",
        "\n",
        "def filter_patches(\n",
        "    slide_id: str, selected_slide_ids: List[str], include_selected: bool\n",
        ") -> bool:\n",
        "  return (\n",
        "      slide_id in selected_slide_ids\n",
        "      if include_selected\n",
        "      else slide_id not in selected_slide_ids\n",
        "  )\n",
        "\n",
        "\n",
        "def get_patches_from_slide_ids(\n",
        "    patches_by_slide: Mapping[str, List[Patch]],\n",
        "    selected_slide_ids: List[str],\n",
        "    include_selected: bool = True,\n",
        ") -> List[Patch]:\n",
        "  patches = []\n",
        "  for slide_id in patches_by_slide:\n",
        "    if filter_patches(slide_id, selected_slide_ids, include_selected):\n",
        "      patches.extend([patch for patch in patches_by_slide[slide_id]])\n",
        "  return patches\n",
        "\n",
        "\n",
        "cancer_slide_patches = build_patches_by_slide_id(cancer_patch_coordiantes)\n",
        "bengin_slide_patches = build_patches_by_slide_id(benign_patch_coordiantes)\n",
        "\n",
        "eval_reserved_slides = select_random_slide_ids(\n",
        "    cancer_slide_patches, EVAL_RESERVED_SLIDES\n",
        ")\n",
        "\n",
        "training_cancer_patches = get_patches_from_slide_ids(\n",
        "    cancer_slide_patches, eval_reserved_slides, include_selected=False\n",
        ")\n",
        "training_benign_patches = get_patches_from_slide_ids(\n",
        "    bengin_slide_patches, eval_reserved_slides, include_selected=False\n",
        ")\n",
        "\n",
        "eval_cancer_patches = get_patches_from_slide_ids(\n",
        "    cancer_slide_patches, eval_reserved_slides, include_selected=True\n",
        ")\n",
        "eval_bengin_patches = get_patches_from_slide_ids(\n",
        "    bengin_slide_patches, eval_reserved_slides, include_selected=True\n",
        ")\n",
        "\n",
        "print(f'Total training benign patches: {len(training_benign_patches)}')\n",
        "print(f'Total training cancer patches: {len(training_cancer_patches)}')\n",
        "print(f'Total eval benign patches: {len(eval_bengin_patches)}')\n",
        "print(f'Total eval cancer patches: {len(eval_cancer_patches)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "823zWUcsIEwD"
      },
      "outputs": [],
      "source": [
        "# @title Initial Helper Functions and Setup\n",
        "\n",
        "dwi = dicom_web_interface.DicomWebInterface(\n",
        "    credential_factory.DefaultCredentialFactory()\n",
        ")\n",
        "\n",
        "\n",
        "def _group_patches_by_series(patches: List[Patch]) -> Iterator[List[Patch]]:\n",
        "  patches_by_series = defaultdict(list)\n",
        "  for patch in patches:\n",
        "    patches_by_series[patch.series_instance_uid].append(patch)\n",
        "  return patches_by_series.values()\n",
        "\n",
        "\n",
        "def generate_embeddings_payload(\n",
        "    patch_count: int, input_patches: List[Patch]\n",
        ") -> Iterator[dicom_slide.DicomPatch]:\n",
        "  selected_patches = random.sample(input_patches, patch_count)\n",
        "  # Group patches by series for efficient processing\n",
        "  for series_patches in _group_patches_by_series(selected_patches):\n",
        "    first_patch = series_patches[0]\n",
        "    path = dicom_path.FromString(\n",
        "        f'https://healthcare.googleapis.com/v1beta1/projects/{DATASET_PROJECT_ID}/locations/{DATASET_LOCATION}/datasets/{DATASET_ID}/dicomStores/{STORE_ID}/dicomWeb/studies/{first_patch.study_instance_uid}/series/{first_patch.series_instance_uid}'\n",
        "    )\n",
        "    slide = dicom_slide.DicomSlide(dwi=dwi, path=path)\n",
        "    level = slide.get_level_by_pixel_spacing(TARGET_PIXEL_SPACING)\n",
        "    for patch in series_patches:\n",
        "      yield slide.get_patch(\n",
        "          level,\n",
        "          patch.x_origin,\n",
        "          patch.y_origin,\n",
        "          width=PATCH_SIZE,\n",
        "          height=PATCH_SIZE,\n",
        "      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58gXSAamr3pJ"
      },
      "source": [
        "##Using the API on Google DICOM store images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KApgNYF_oqWl"
      },
      "outputs": [],
      "source": [
        "# @title Define Cloud Endpoint used to Generate Embeddings.\n",
        "\n",
        "endpoint = patch_embedding_endpoints.V2PatchEmbeddingEndpoint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZB-kvlgHZjs"
      },
      "outputs": [],
      "source": [
        "# @title Generate Embeddings for the patches in the Training and Eval sets\n",
        "# Note: May take approximately 5 Minutes\n",
        "\n",
        "list_of_patch_iterators = [\n",
        "    generate_embeddings_payload(\n",
        "        patch_count=EVAL_CANCER_PATCH_COUNT, input_patches=eval_cancer_patches\n",
        "    ),\n",
        "    generate_embeddings_payload(\n",
        "        patch_count=EVAL_BENIGN_PATCH_COUNT,\n",
        "        input_patches=eval_bengin_patches,\n",
        "    ),\n",
        "    generate_embeddings_payload(\n",
        "        patch_count=TRAINING_CANCER_PATCH_COUNT,\n",
        "        input_patches=training_cancer_patches,\n",
        "    ),\n",
        "    generate_embeddings_payload(\n",
        "        patch_count=TRAINING_BENIGN_PATCH_COUNT,\n",
        "        input_patches=training_benign_patches,\n",
        "    ),\n",
        "]\n",
        "\n",
        "\n",
        "def _get_patch_embeddings(\n",
        "    patches: Iterator[dicom_slide.DicomPatch],\n",
        ") -> List[patch_embedding_types.EmbeddingResult]:\n",
        "  return list(patch_embedding.generate_patch_embeddings(endpoint, patches))\n",
        "\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
        "  results = list(executor.map(_get_patch_embeddings, list_of_patch_iterators))\n",
        "eval_cancer_embeddings = results[0]\n",
        "eval_begnin_embeddings = results[1]\n",
        "training_cancer_embeddings = results[2]\n",
        "training_begnin_embeddings = results[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAGdhqtbuPHs"
      },
      "source": [
        "## Train and Evaluate Linear Probe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LuZpl-p8prK"
      },
      "outputs": [],
      "source": [
        "# @title Organize embeddings for ML training\n",
        "\n",
        "\n",
        "def get_embeddings(\n",
        "    embedding_results: Sequence[patch_embedding_types.EmbeddingResult],\n",
        ") -> np.ndarray:\n",
        "  \"\"\"Returns numpy array of embeddings returned in embedding results list.\"\"\"\n",
        "  return np.array([e.embedding for e in embedding_results])\n",
        "\n",
        "\n",
        "def concatenate_series_ids(\n",
        "    embedding_results: List[patch_embedding_types.EmbeddingResult],\n",
        ") -> np.ndarray:\n",
        "  \"\"\"Concatenates instance UIDs into a NumPy array.\"\"\"\n",
        "  # Assume there is one instance uid per series.\n",
        "  return np.asarray([e.patch.source.path.series_uid for e in embedding_results])\n",
        "\n",
        "\n",
        "def concatenate_training_data_and_build_training_labels(\n",
        "    cancer: Sequence[patch_embedding_types.EmbeddingResult],\n",
        "    benign: Sequence[patch_embedding_types.EmbeddingResult],\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "  \"\"\"Concatenate cancer and benign examples into and generate label data.\"\"\"\n",
        "  data = np.concatenate([get_embeddings(cancer), get_embeddings(benign)])\n",
        "  labels = np.concatenate((np.ones(len(cancer)), np.zeros(len(benign))))\n",
        "  return data, labels\n",
        "\n",
        "\n",
        "# Embeddings and training lables\n",
        "training_embeddings, training_labels = (\n",
        "    concatenate_training_data_and_build_training_labels(\n",
        "        training_cancer_embeddings, training_begnin_embeddings\n",
        "    )\n",
        ")\n",
        "training_ids = np.concatenate([\n",
        "    concatenate_series_ids(training_cancer_embeddings),\n",
        "    concatenate_series_ids(training_begnin_embeddings),\n",
        "])\n",
        "\n",
        "# Generate evaluation embeddings and labels\n",
        "eval_embeddings, eval_labels = (\n",
        "    concatenate_training_data_and_build_training_labels(\n",
        "        eval_cancer_embeddings, eval_begnin_embeddings\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYWil5ur8ruZ"
      },
      "outputs": [],
      "source": [
        "# Train a linear classifier using the embeddings\n",
        "\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "  warnings.simplefilter('ignore')\n",
        "  clf_pipeline = sklearn.pipeline.Pipeline([\n",
        "      ('scaler', sklearn.preprocessing.StandardScaler()),\n",
        "      (\n",
        "          'logreg',\n",
        "          sklearn.model_selection.GridSearchCV(\n",
        "              sklearn.linear_model.LogisticRegression(\n",
        "                  random_state=0,\n",
        "                  multi_class='ovr',\n",
        "                  verbose=False,\n",
        "              ),\n",
        "              cv=sklearn.model_selection.StratifiedGroupKFold(n_splits=5).split(\n",
        "                  training_embeddings, y=training_labels, groups=training_ids\n",
        "              ),\n",
        "              param_grid={'C': np.logspace(start=-4, stop=4, num=10, base=10)},\n",
        "              scoring='roc_auc_ovr',\n",
        "              refit=True,\n",
        "          ),\n",
        "      ),\n",
        "  ]).fit(training_embeddings, training_labels)\n",
        "\n",
        "  test_predictions = clf_pipeline.predict_proba(eval_embeddings)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JykI-foV8vuL"
      },
      "outputs": [],
      "source": [
        "# Evaluate the linear classifiers performance using the eval patches\n",
        "\n",
        "sklearn.metrics.roc_auc_score(eval_labels, test_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS-6ahz3i2Hn"
      },
      "outputs": [],
      "source": [
        "# @title Plot the ROC Curve\n",
        "\n",
        "display = sklearn.metrics.RocCurveDisplay.from_predictions(\n",
        "    eval_labels, test_predictions, name=\"Tumor Classifier\"\n",
        ")\n",
        "display.ax_.set_title(\"ROC of Tumor Classifier\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORAsWcLVcERu"
      },
      "outputs": [],
      "source": [
        "# @title Find Youden's index for threshold selection\n",
        "\n",
        "thresholds = np.linspace(0, 1, 100)\n",
        "sensitivities = []\n",
        "specificities = []\n",
        "for threshold in thresholds:\n",
        "  predictions = test_predictions > threshold\n",
        "  sensitivities.append(sklearn.metrics.recall_score(eval_labels, predictions))\n",
        "  specificities.append(\n",
        "      sklearn.metrics.recall_score(eval_labels == 0, predictions == 0)\n",
        "  )\n",
        "index = np.argmax(np.array(sensitivities) + np.array(specificities))\n",
        "best_threshold = thresholds[index]\n",
        "sens = sensitivities[index]\n",
        "spec = specificities[index]\n",
        "print(\n",
        "    f\"Best threshold: {round(best_threshold,2)}. Sensitivity is\"\n",
        "    f\" {round(sens*100,2)}% and Specificity is {round(spec*100,2)}% \"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIlXrLMQjeeb"
      },
      "outputs": [],
      "source": [
        "# @title Show the results in a table\n",
        "eval_embeddings_obj = eval_cancer_embeddings + eval_begnin_embeddings\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    {'ground_truth': eval_labels, 'model_score': test_predictions}\n",
        ")\n",
        "df['tumor_prediction'] = df['model_score'] > best_threshold\n",
        "df['embeddings'] = [e.embedding for e in eval_embeddings_obj]\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxfGFAyJVf5S"
      },
      "outputs": [],
      "source": [
        "# @title Visualize True Positives\n",
        "def display_results(\n",
        "    tumor_prediction: bool, ground_truth: int, title: str\n",
        ") -> None:\n",
        "  df_tp = (\n",
        "      df[\n",
        "          (df['tumor_prediction'] == tumor_prediction)\n",
        "          & (df['ground_truth'] == ground_truth)\n",
        "      ]\n",
        "      .sort_values('model_score', ascending=False)\n",
        "      .head(5)\n",
        "  )\n",
        "  for index, row in df_tp.iterrows():\n",
        "    print(index)\n",
        "    print(f'model score is {row.model_score}')\n",
        "    render_patch_from_embedding(eval_embeddings_obj[index].patch, title)\n",
        "\n",
        "\n",
        "display_results(True, 1, 'True Positive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jvb8-wbZVtkS"
      },
      "outputs": [],
      "source": [
        "# @title Visualize True Negatives\n",
        "display_results(False, 0, 'True Negative')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCB6sfxvjnru"
      },
      "outputs": [],
      "source": [
        "# @title Visualize False Positives\n",
        "display_results(True, 0, 'False Positive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laWdwj_7jp2K"
      },
      "outputs": [],
      "source": [
        "# @title Visualize False Negatives\n",
        "display_results(False, 1, 'False Negative')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOucQL10vTkF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/third_party/health_foundations/path_foundation/notebooks/dicom_linear-classifier-demo.ipynb?workspaceId=philbrik:dicom_embeddings::citc",
          "timestamp": 1730133637331
        },
        {
          "file_id": "/piper/depot/google3/third_party/health_foundations/path_foundation/notebooks/dicom_linear-classifier-demo.ipynb?workspaceId=philbrik:dicom_embeddings::citc",
          "timestamp": 1729780610544
        },
        {
          "file_id": "/piper/depot/google3/third_party/health_foundations/path_foundation/notebooks/dicom_linear-classifier-demo.ipynb?workspaceId=philbrik:dicom_embeddings::citc",
          "timestamp": 1729780302418
        },
        {
          "file_id": "/piper/depot/google3/third_party/health_foundations/path_foundation/notebooks/dicom_linear-classifier-demo.ipynb",
          "timestamp": 1729780229806
        },
        {
          "file_id": "/piper/depot/google3/third_party/health_foundations/pathology/v2/ez_wsi_simple-linear-classifier-demo.ipynb?workspaceId=philbrik:dicom_linear_classifier::citc",
          "timestamp": 1729694426511
        },
        {
          "file_id": "/piper/depot/google3/third_party/health_foundations/pathology/v2/ez_wsi_simple-linear-classifier-demo.ipynb?cl=679646156",
          "timestamp": 1729660844199
        },
        {
          "file_id": "/piper/depot/google3/third_party/health_foundations/pathology/v2/simple-linear-classifier-demo.ipynb?workspaceId=philbrik:v2_colab_port::citc",
          "timestamp": 1727280808069
        },
        {
          "file_id": "/piper/depot/google3/third_party/health_foundations/pathology/simple-linear-classifier-demo.ipynb?workspaceId=philbrik:embedding_example::citc",
          "timestamp": 1726884269654
        },
        {
          "file_id": "/piper/depot/google3/third_party/health_foundations/pathology/simple-linear-classifier-demo.ipynb",
          "timestamp": 1726882804846
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}